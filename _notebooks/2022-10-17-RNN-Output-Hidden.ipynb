{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# torch RNN/LSTM/GRU의 output과 hidden \n",
    "> \n",
    "\n",
    "- toc: true\n",
    "- Badges: true\n",
    "- comments: true\n",
    "- date: 2022-10-17\n",
    "- last-modified-at: 2022-10-17 \n",
    "- categories: [TIL]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`torch.nn.RNN` `torch.nn.LSTM` `torch.nn.GRU`은 `forward`의 결과물로 두 벡터를 돌려준다. pytorch의 공식 doc에 의하면 `output`과 `h_n`인데, 만드는 모델, 과제에 따라 사용해야 하는 벡터가 다르다. 지금까지 주로 huggingface 라이브러리를 썼기 때문에 torch.nn의 RNN/LSTM/GRU를 쓸 일은 별로 없었지만 아직도 양뱡향 LSTM 모델등은 일부 사용하는 경우가 있기에 이번 기회에 항상 헷갈렸던 `output`과 `'h_n`의 차이점에 대해 알아봤다.\n",
    "\n",
    "> Note: `nn.LSTM` 은 `output`과 hidden_state, cell state가 담긴 튜플 `(h_n, c_n)`를 리턴한다.\n",
    "\n",
    "아래 설명은 input이 `torch.nn.utils.rnn.PackedSequence` 객체로 되어 있다고 가정한다. 편이상 RNN/LSTM/GRU는 RNN으로 통칭한다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## output과 h_n의 차이\n",
    "\n",
    "* **output**\n",
    "\n",
    "> shape: (seq_len, Batch_size, D * hidden_size) (Batch_first=False일 시)\n",
    "\n",
    "`output`은 *모든 time step*의 *final layer*의 hidden state들이다. 즉 주어진 문장 내 모든 토큰들의 마지막 hidden state이다. 만약 양방향 RNN이면 각 토큰별 순방향과 역방향시 h_t시 concat되어 리턴되므로 마지막 차원의 크기는 2 * `hidden_size`이다.\n",
    "\n",
    "* **h_n**\n",
    "\n",
    "> shape: (D * n_layers, Batch_size, hidden_size)\n",
    "\n",
    "`h_n`는 *마지막 time step (token)*의 *모든 layer*의 hidden state이다. `D=2` 즉 양뱡향 RNN 모델일 때 `h_n`의 짝수번째 벡터는 순방향, 홀수번째 벡터는 역방향 layer의 최종 결과물이다. \n",
    "\n",
    "|          |    output    | h_n |\n",
    "|----------|:------------:|:---:|\n",
    "| layer    |  final only  | all |\n",
    "| timestep | 1, 2, ..., t |  t  |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## output과 h_n의 관계\n",
    "\n",
    "레이어 수가 하나인 단방향 RNN의 경우에만 `h_n`은 `output`의 부분집합이다. 정확히 말해 마지막 토큰의 벡터인 `output[-1]`이 `h_n`이다. 그러나 레이어 수가 2 이상이고, 양뱡향인 RNN의 경우 `output`과 `h_n`의 정확한 관계를 파악하기가 어렵다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 7])"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "# pad_token의 id: 0\n",
    "X = [[12, 23, 1, 16, 59, 6, 37],\n",
    "     [6,  3,  2, 76, 31, 0, 0],\n",
    "     [12, 6,  1,  0,  0, 0, 0],\n",
    "     [11, 21, 18, 1,  0, 0, 0]]\n",
    "\n",
    "X = torch.tensor(X, dtype=torch.long)\n",
    "X.shape # (batch_size, seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정렬 후: tensor([[12, 23,  1, 16, 59,  6, 37],\n",
      "        [ 6,  3,  2, 76, 31,  0,  0],\n",
      "        [11, 21, 18,  1,  0,  0,  0],\n",
      "        [12,  6,  1,  0,  0,  0,  0]])\n",
      "패딩 토큰을 제외한 실토큰 개수: tensor([7, 5, 4, 3])\n"
     ]
    }
   ],
   "source": [
    "# pad_token을 제외한 실제 token들의 len\n",
    "input_lens = torch.tensor([torch.max(row.nonzero())+1 for row in X])\n",
    "input_lens, indices = input_lens.sort(descending=True)\n",
    "X_sorted = X[indices]\n",
    "\n",
    "print(f'정렬 후: {X_sorted}')\n",
    "print(f'패딩 토큰을 제외한 실토큰 개수: {input_lens}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size=256\n",
    "\n",
    "class RNNModel(nn.Module):\n",
    "  def __init__(self, rnn_type):\n",
    "    super(RNNModel, self).__init__()\n",
    "\n",
    "    self.embedding = nn.Embedding(100, 128)\n",
    "    self.num_layers = 2\n",
    "    self.hidden_size = hidden_size\n",
    "\n",
    "    self.rnn = getattr(nn, rnn_type)(\n",
    "        input_size=128, \n",
    "        hidden_size=self.hidden_size,\n",
    "        num_layers=2,\n",
    "        bidirectional=True,\n",
    "    )\n",
    "\n",
    "  def forward(self, Batch, Batch_lens):  # Batch: (batch_size, seq_len), Batch_lens: (batch_size)\n",
    "    # d_w: word emBedding size\n",
    "    batch_emb = self.embedding(Batch)  # (batch_size, seq_len, d_w)\n",
    "    batch_emb = batch_emb.transpose(0, 1)  # (seq_len, batch_size, d_w)\n",
    "\n",
    "    packed_input = pack_padded_sequence(batch_emb, Batch_lens)\n",
    "    h_0 = torch.zeros((self.num_layers * 2, Batch.shape[0], self.hidden_size))  # (num_layers*num_dirs, batch_size, d_h) = (4, batch_size, d_h)\n",
    "    packed_output, h_n = self.rnn(packed_input, h_0)  # h_n: (4, batch_size, d_h)\n",
    "    output = pad_packed_sequence(packed_output)[0]  # outputs: (seq_len, batch_size, 2*d_h)\n",
    "\n",
    "    return output, h_n, packed_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output shape (seq_len, batch_size, 2*hidden_size) == torch.Size([7, 4, 512])\n",
      "h_n shape (2*num_layers, batch_size, hidden_size) == torch.Size([4, 4, 256])\n"
     ]
    }
   ],
   "source": [
    "# 양방향 RNN이고, batch_size = False일 때\n",
    "\n",
    "rnn = RNNModel(rnn_type='GRU')\n",
    "output, h_n, packed_outputs = rnn(X_sorted, input_lens)\n",
    "print(f'output shape (seq_len, batch_size, 2*hidden_size) == {output.shape}')\n",
    "print(f'h_n shape (2*num_layers, batch_size, hidden_size) == {h_n.shape}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **순방향**\n",
    "\n",
    "순방향시 마지막 time-step은 문장 맨 마지막 토큰이며, 그 값은 `output` 마지막 차원의 첫 `hidden_size`개의 원소와 같다 (순방향+역방향을 붙인 벡터이기 때문에).\n",
    "\n",
    "* **역방향**\n",
    "\n",
    "역방향시에는 마지막 time-step이 문장 맨 처음 토큰이며, 그 값은 `output` 마지막 차원내 뒤에서부터 `hidden_size`개의 원소와 같다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(True)\n",
      "tensor(True)\n"
     ]
    }
   ],
   "source": [
    "# forward: last time-step (i.e. last token), last-layer\n",
    "print(torch.eq(output[-1, 0, :hidden_size], h_n[2, 0, :]).all())\n",
    "\n",
    "# backward: last time-step (i.e. first token), last-layer\n",
    "print(torch.eq(output[0, 0, hidden_size:], h_n[3, 0, :]).all())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`output`과 `h_n`의 관계를 일반화 해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At 0th batch output & h_n --> equal\n",
      "At 1th batch output & h_n --> equal\n",
      "At 2th batch output & h_n --> equal\n",
      "At 3th batch output & h_n --> equal\n"
     ]
    }
   ],
   "source": [
    "# backward \n",
    "batch_size = output.shape[1]\n",
    "for batch_idx in range(batch_size):\n",
    "    if torch.eq(output[0, batch_idx, hidden_size:], h_n[3, batch_idx, :]).all().item():\n",
    "        print(f'At {batch_idx}th batch output & h_n --> equal')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "역방향 rnn layer의 경우 `output`의 첫번째 토큰(`0`)은 모든 배치에서 `[hidden_size : ]` 원소들과 `h_n[3]`의 원소들은 일치한다 (앞서 말했듯 `h_n`은 모든 레이어의 마지막 time-step의 hidden_state를 리턴하는데, 첫 레이어의 순방향, 첫 레이어의 역방향, 두번째 레이어의 순방향, 두번째 레이어의 역방향시 마지막 hidden state들을 차례차례 리턴한다).  \n",
    "\n",
    "순방향의 rnn layer는 어떨까?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At 0th batch output & h_n --> equal\n",
      "At 1th batch output & h_n --> NOT equal\n",
      "At 2th batch output & h_n --> NOT equal\n",
      "At 3th batch output & h_n --> NOT equal\n"
     ]
    }
   ],
   "source": [
    "# forward\n",
    "for batch_idx in range(batch_size):\n",
    "    if torch.eq(output[-1, batch_idx, :hidden_size], h_n[2, batch_idx, :]).all().item():\n",
    "        print(f'At {batch_idx}th batch output & h_n --> equal')\n",
    "    else:\n",
    "        print(f'At {batch_idx}th batch output & h_n --> NOT equal')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "마지막 토큰의 hidden state 즉 `output[-1, batch_idx, :hidden_size]`를 했음에도 몇몇 배치에서는 예상과 다르게 나왔다. 문제를 알아보기 위해 다음 코드를 돌려봤다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For 6th token at 0th batch output & h_n --> equal\n",
      "For 4th token at 1th batch output & h_n --> equal\n",
      "For 3th token at 2th batch output & h_n --> equal\n",
      "For 2th token at 3th batch output & h_n --> equal\n"
     ]
    }
   ],
   "source": [
    "# forward\n",
    "for batch_idx in range(batch_size):\n",
    "    for token_idx in range(output.shape[0]):\n",
    "        if torch.eq(output[token_idx, batch_idx, :hidden_size], h_n[2, batch_idx, :]).all().item():\n",
    "            print(f'For {token_idx}th token at {batch_idx}th batch output & h_n --> equal')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "각 배치마다 `output`과 `h_n`이 겹치는 토큰 인덱스가 다른데, 정렬한 인풋의 (패딩 토큰을 제외한) 실토큰의 개수와 일치함을 알 수 있다. 이는 마지막 time step의 hidden state라고 하는 `h_n`의 경우 (패딩 토큰이 없는) 실제 문장의 마지막 토큰의 hidden state을 담기 때문이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정렬 후: tensor([[12, 23,  1, 16, 59,  6, 37],\n",
      "        [ 6,  3,  2, 76, 31,  0,  0],\n",
      "        [11, 21, 18,  1,  0,  0,  0],\n",
      "        [12,  6,  1,  0,  0,  0,  0]])\n",
      "패딩 토큰을 제외한 실토큰 개수: tensor([7, 5, 4, 3])\n"
     ]
    }
   ],
   "source": [
    "# 위에서 만든 정렬한 인풋\n",
    "print(f'정렬 후: {X_sorted}')\n",
    "print(f'패딩 토큰을 제외한 실토큰 개수: {input_lens}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('.venv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5bebc21204ab4a130559607128dfa87d7088e010f5924ba041b3803c15c5db1e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
