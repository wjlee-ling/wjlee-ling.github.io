---
title: Weight Initialization
description: He & Kaiming Initialization
search: true
toc: true
date: 2023-03-22
categories: 
    - TIL
tags: 
    - weight initialization
---

Pytorch나 Keras를 사용하여 딥러닝 모델을 만들다보면 어떤 타입의 레이어를 쓸지, 또 레이어의 차원은 어떻게할지, 배치 사이즈는 어떻게 할지 등 하이퍼파라미터 세팅 위주로 신경쓰게 된다. 정작 중요한 파라미터, 즉 실제로 학습이 되는 벡터들은 어떻게 초기화해야할지 신경 안쓰고 곧바로 `nn.Linear`를 쌓게 된다. 그런데 이 하이퍼파라미터 초기화가 학습 속도는 물론, 모델 최종 성능에도 영향을 미칠 수 있는 만큼 확실하게 정리해 본다. 
항상 그렇듯 강의나 블로그 글을 그대로 한국어로 (번역도 아닌) 해석한 포스트가 아니라, 여러 강의 자료와 논문, 블로그 글을 읽은 뒤 나 나름대로 정리한 내용이다.

## 참고
1. [Stanford CS231n--Training Nerual Networks 1](https://youtu.be/wEoyxE0GP2M)


## activation

효과적인 가중치 설정법을 이해하려면 순전파와 역전파를 복습할 필요가 있다. 우선 아핀 레이어에서 순전파는 $$ z = \bm{XW} $$ 로 이뤄지고 역전파시 로컬 기울기는 $$ \frac{\partial{z}}{\partial{\bm{W}}} = \bm{X} $$ 이다. 가중치 $$ \bm{W} $$ 값과 상관없이 레이어의 인풋 $$ \bm{X} $$ 값으로만 가중치 업데이트가 된다는 건데 왜 가중치 초기값을 고민해야하는지 의문이 들어야 한다. 그 이유는 딥러닝 모델의 힘은 레이어를 2층 이상 쌓는 데서 나오고, 순전파 시 이번 레이어의 아웃풋이 다음 레이어의 인풋이기 때문이다. 즉 역전파 시 이번 레이어층의 로컬 기울기의 값을 좌지우지하는 $ \bm{W} $ 는 이전 레이어의 아웃풋으로 이 아웃풋은 $ \sigma (\bm{XW}) $ 이기 때문에 결국 가중치가 중요한 것이다.

## zero-centering

현재 많이 사용되는 He 초기화나 Kaiming 초기화는 모두 평균이 0인 분포에서 가중치를 끌어오는 방법이다. 왜 평균이 0인게 중요할까? 직관적으로도 양수로만 혹은 음수로만 가중치를 세팅하는 것보다는 양수, 음수, 0 섞어서 초기화하는 게 좋을 것 같긴 한데 수학적으로도 그럴까?
만약에 선형 레이어의 인풋 벡터의 모든 요소가 양수라면 어떻게 될까? 이 레이어의 로컬 기울기는 $$ \bm{W} $$ 이므로   